---
title: "Quantifying Deep Learning Model Uncertainty in Conformal Prediction"
collection: publications
category: conferences
permalink: /publication/2023-cp-uncertainty-aaai-ss
excerpt: "Probabilistic quantification of uncertainty derived from conformal prediction sets."
date: 2023-06-01
venue: "AAAI Symposium Series (Human AI)"
paperurl: "https://ojs.aaai.org/index.php/AAAI-SS/article/view/27492"
pdf: "https://ojs.aaai.org/index.php/AAAI-SS/article/download/27492/27265/31543"
doi: "https://doi.org/10.1609/aaaiss.v1i1.27492"
arxiv: "https://arxiv.org/abs/2306.00876"
citation: 'Karimi, H., & Samavi, R. (2023). “Quantifying Deep Learning Model Uncertainty in Conformal Prediction.” Proceedings of the AAAI Symposium Series, 1(1), 142–148.'
---

<div class="justify-text">
Precise estimation of predictive uncertainty in deep neural networks is a critical requirement for reliable decisionmaking in machine learning and statistical modeling, particularly in the context of medical AI. Conformal Prediction (CP) has emerged as a promising framework for representing the model uncertainty by providing well-calibrated confidence levels for individual predictions. However, the quantification of model uncertainty in conformal prediction remains an active research area, yet to be fully addressed. In this paper, we explore state-of-the-art CP methodologies and their theoretical foundations. We propose a probabilistic approach in quantifying the model uncertainty derived from the produced prediction sets in conformal prediction and provide certified boundaries for the computed uncertainty. By doing so, we allow model uncertainty measured by CP to be compared by other uncertainty quantification methods such as Bayesian (e.g., MC-Dropout and DeepEnsemble) and Evidential approaches.
</div>
<br><br>
